<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>(2 条消息) 为什么几乎所有教科书上对微分的讲解都不明不白？ - 知乎</title>
  <meta name="created-at" content="1767113799968">
  <meta name="updated-at" content="1767113799968">
  <meta name="source-type" content="zhihu">
  <meta name="source-url" content="https://www.zhihu.com/question/438795295/answer/1989001841367864615">
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      max-width: 800px;
      margin: 40px auto;
      padding: 20px;
      background: #f5f5f5;
      color: #333;
    }
    h1 {
      color: #333;
      border-bottom: 2px solid #10a37f;
      padding-bottom: 10px;
    }
    .message-block {
      margin: 20px 0;
      padding: 16px;
      border-radius: 12px;
    }
    .role-user {
      background: #e8f4fd;
      border-left: 4px solid #3b82f6;
    }
    .role-ai {
      background: #f0fdf4;
      border-left: 4px solid #10b981;
    }
    .message-role {
      font-weight: 700;
      margin-bottom: 8px;
    }
    .role-user .message-role { color: #2563eb; }
    .role-ai .message-role { color: #059669; }
    .message-content {
      line-height: 1.6;
    }
    pre {
      background: #1a1a1a;
      color: #e5e5e5;
      padding: 16px;
      border-radius: 8px;
      overflow-x: auto;
    }
    code {
      font-family: 'Consolas', 'Monaco', monospace;
    }
    img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
    }
  </style>
</head>
<body>
  <h1>(2 条消息) 为什么几乎所有教科书上对微分的讲解都不明不白？ - 知乎</h1>
  <div class="message-block role-ai">
          <div class="message-role">AI</div>
          <div class="message-content"><h1>为什么几乎所有教科书上对微分的讲解都不明不白？</h1>
<div class="css-376mun"><span id="content"><span class="RichText ztext CopyrightRichText-richText css-10o75c2" options="[object Object]" itemprop="text"><p data-first-child="" data-pid="IxYXNyrI">因为几乎所有的工科微积分教材（同济版为代表）在讲微分的时候，都在试图掩盖一个事实：<b>导数和微分，根本就是两码事，但在低维空间里它们恰好长得一样。</b></p><p data-pid="3gMmb1Mb">这就好比在二维平面上，复数和向量都能用 <span class="math-tex" data-tex="(x, y)">$(x, y)$</span> 表示，你就误以为它俩是一回事。</p><p data-pid="Fdv-qCrs">教科书最大的误导在于，它太想让你把微分理解成 一个很小的数。</p><p data-pid="RDV39O1l">你想想，牛顿和莱布尼茨当年搞微积分的时候，确实是把 <span class="math-tex" data-tex="dx">$dx$</span> 当作一个 无穷小 的量。这就好比早期的汇编语言，直接操作内存地址。这种理解非常直观，物理学家特别喜欢。你把它切成无数小块，每一块就是 <span class="math-tex" data-tex="dx">$dx$</span>。</p><p data-pid="LqqRqUfK">但是这玩意儿在数学上是不严谨的。如果你承认有 无穷小 这种实数存在，那 <span class="math-tex" data-tex="1">$1$</span> 除以无穷小是啥？第一次数学危机就是这么来的。</p><p data-pid="p_sGv_BV">后来柯西他们搞出了 <span class="math-tex" data-tex="\epsilon-\delta">$\epsilon-\delta$</span> 语言，把大厦地基重修了一遍。按理说，这时候 <span class="math-tex" data-tex="dx">$dx$</span> 作为 无穷小量 的概念就该被废除了。但是！莱布尼茨发明的那套符号系统 <span class="math-tex" data-tex="dy/dx">$dy/dx$</span> 实在是太好用了，好用到哪怕逻辑不对，大家也舍不得扔。</p><p data-pid="8YM1nBzP">于是教材就开始 骑墙 了。</p><p data-pid="1tEvGUxp">它先用极限严格定义了导数 <span class="math-tex" data-tex="f'(x)">$f'(x)$</span>。这一步是严谨的。 然后它定义微分 <span class="math-tex" data-tex="dy = f'(x)\Delta x">$dy = f'(x)\Delta x$</span>。注意，这里突然把 <span class="math-tex" data-tex="\Delta x">$\Delta x$</span> 换成了 <span class="math-tex" data-tex="dx">$dx$</span>，告诉你对于自变量，<span class="math-tex" data-tex="dx = \Delta x">$dx = \Delta x$</span>。 这其实是一种 强行定义。</p><p data-pid="WSs7-2uP">这就好比我在写代码，先定义了一个严谨的类 Class A，然后为了兼容旧代码，我硬生生写了个宏定义 <code>#define B A</code>，然后告诉你 B 就是 A。</p><p data-pid="qORfWW3h">教材这么做是为了让你在做积分的时候，能顺理成章地写出 <span class="math-tex" data-tex="\int f(x) dx">$\int f(x) dx$</span>，并且在使用换元法的时候，能像操作分数一样操作 <span class="math-tex" data-tex="dy">$dy$</span> 和 <span class="math-tex" data-tex="dx">$dx$</span>。</p><p data-pid="ITqmqclr">比如 <span class="math-tex" data-tex="y = y(u), u = u(x)">$y = y(u), u = u(x)$</span>，那么 <span class="math-tex" data-tex="\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}">$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$</span>。 如果不把 <span class="math-tex" data-tex="dy">$dy$</span> 和 <span class="math-tex" data-tex="dx">$dx$</span> 看作是可以独立运算的量，这个链式法则的记忆成本会高很多。</p><p data-pid="VEpXjqZH">所以，教材的策略是：<span class="highlight-wrap other has-comments" data-highlight-id="1989101814973502151" data-highlight-split-type="both" data-highlight-id-extra="">为了让你 算得爽，</span>逻辑上就 糊弄 一下。对于99%的工科生来说，这够用了。<span class="highlight-wrap other has-comments" data-highlight-id="1989124785150654229" data-highlight-split-type="both" data-highlight-id-extra="">因为他们以后只需要算算梁的弯曲，或者电路的响应，不需要去深究流形上的切丛</span>。</p><p data-pid="OrzKMTdu"><span class="highlight-wrap other has-comments" data-highlight-id="1989138550411790081" data-highlight-split-type="both" data-highlight-id-extra="">但是对于你这种在这个问题上卡住的人，说明你的数学直觉非常敏锐。</span>你察觉到了这个系统里的 Bug。</p><p data-pid="gEqayesU">这里我必须要推荐一本神书，《卓里奇数学分析》。它是俄罗斯教材的巅峰。相比于国内教材的语焉不详，卓里奇从一开始就把 <b>现代数学</b> 的观点引入进来。或者是找一些专门针对国外数学体系的经典教材来互相对照，很多国外教材，特别是分析学这些，他们都不会把你当傻子，而是明明白白告诉你微分是线性映射。</p><p data-pid="OHGmbEmv"><a href="https://mp.weixin.qq.com/s/N3ih904eGewoWD7Bs09x8Q" class=" wrap external" target="_blank" rel="nofollow noreferrer">76本国外经典教材，全方位覆盖代数、几何、分析、概率等核心领域</a></p><p data-pid="9PuuG_1b">咱们把视角拉回到现代，拉回到数据科学和算法。</p><p data-pid="WpsKcx62">在机器学习里，我们天天搞梯度下降，搞反向传播。你见过哪个算法工程师在代码里写 无穷小量 吗？没有。</p><p data-pid="sF6IjNZ8">在现代数学和计算机科学里，<b>微分的本质是 线性逼近</b>。</p><p data-pid="kAAyozS6">你把函数 <span class="math-tex" data-tex="f">$f$</span> 想象成一个复杂的非线性变换。比如一个深层的神经网络，或者一个复杂的曲面。 我们在某一点 <span class="math-tex" data-tex="x_0">$x_0$</span> 附近，想要研究这个函数。非线性太难了，咱们搞不定。怎么办？ 我们试图用一个 线性函数（直线、平面、超平面） 来模仿它。</p><p data-pid="2MqtlFZJ"><b>微分，就是这个最佳的线性模拟器。</b></p><p data-pid="mGi-v5pw">用公式说人话： <span class="math-tex" data-tex="f(x_0 + h) - f(x_0) = L(h) + o(h)">$f(x_0 + h) - f(x_0) = L(h) + o(h)$</span></p><p data-pid="ELYJnL8m">这里 <span class="math-tex" data-tex="h">$h$</span> 是一个小位移（向量）。 <span class="math-tex" data-tex="L">$L$</span> 是一个线性变换（Linear Map）。 <span class="math-tex" data-tex="o(h)">$o(h)$</span> 是高阶无穷小，也就是误差。</p><p data-pid="psKtR2D3"><b>这个线性变换 <span class="math-tex" data-tex="L">$L$</span>，就是函数在 <span class="math-tex" data-tex="x_0">$x_0$</span> 处的微分，记作 <span class="math-tex" data-tex="df_{x_0}">$df_{x_0}$</span>。</b></p><p data-pid="qdKrjG6c">请注意，这里 <span class="math-tex" data-tex="df">$df$</span> 不是一个数，它是一个 <b>函数</b>，一个 <b>线性算子</b>。它吃进去一个向量 <span class="math-tex" data-tex="h">$h$</span>，吐出来一个数值，告诉你函数值大概变了多少。</p><p data-pid="5wSCwSSE">这才是 d 的真面目：<b>它是把非线性函数局部线性化的那个 算子。</b></p><p data-pid="K1EbB0Nh">回到你问的 <span class="math-tex" data-tex="d">$d$</span> 是什么。 <span class="math-tex" data-tex="d">$d$</span> 是一个 <b>算子</b>（Operator），它作用在函数 <span class="math-tex" data-tex="f">$f$</span> 上，生成了一个新的东西 <span class="math-tex" data-tex="df">$df$</span>（微分形式）。 而 <span class="math-tex" data-tex="dx">$dx$</span> 是什么？在现代观点下，<span class="math-tex" data-tex="x">$x$</span> 也是一个函数（坐标函数），<span class="math-tex" data-tex="dx">$dx$</span> 就是坐标函数的微分。</p><p data-pid="4bCfwD5E">这就解释了为什么 <span class="math-tex" data-tex="dy = f'(x)dx">$dy = f'(x)dx$</span>。 这不是除法，这是两个 线性映射 的相等关系。 左边是 <span class="math-tex" data-tex="dy">$dy$</span>（关于 <span class="math-tex" data-tex="y">$y$</span> 的线性逼近），右边是 <span class="math-tex" data-tex="f'(x)">$f'(x)$</span>（一个系数，或者矩阵）乘以 <span class="math-tex" data-tex="dx">$dx$</span>（关于 <span class="math-tex" data-tex="x">$x$</span> 的线性逼近）。 它们作用在同一个向量 <span class="math-tex" data-tex="h">$h$</span> 上时，结果是一样的。</p><p data-pid="MatHyuBX">如果你搞懂了这一点，你对微积分的理解就超过了90%的大学生。</p><p data-pid="NepXvbLN">这里插一个资源推荐。如果你想从 线性代数 的角度彻底搞懂微积分（这其实是正道），一定要看 <b>3Blue1Brown</b> 的视频。他的几何直觉是顶级的，特别是他讲 <a href="https://www.bilibili.com/video/BV1ys411472E/?spm_id_from=333.1387.homepage.video_card.click&amp;vd_source=4c918a043c22277de12ecfb2ce4883dc" data-draft-title="【合集】线性代数的本质 中英双语" class=" wrap external" target="_blank" rel="nofollow noreferrer">【合集】线性代数的本质 中英双语</a>，能让你明白矩阵不仅仅是数字的方阵，而是空间的变换。看完这个，你再配合 Gilbert Strang 的公开课，或者参考 <a href="https://mp.weixin.qq.com/s/UcdvTSsjKawoT7TbzbwehQ" data-draft-title="3Blue1Brown线性代数笔记 中英合集" class=" wrap external" target="_blank" rel="nofollow noreferrer">3Blue1Brown线性代数笔记 中英合集</a> 来复盘，你会发现微积分里的雅可比矩阵瞬间就亲切了。</p><p data-pid="iOpgzitw">接下来咱们讲点你在普通教材里死活找不到答案的部分。</p><p data-pid="0_ZcG9wG">你问 <span class="math-tex" data-tex="dx">$dx$</span> 是未知数还是文字？</p><p data-pid="gjRgJ_iz">在实数域的一元微积分里，教材把它糊弄成一个增量 <span class="math-tex" data-tex="\Delta x">$\Delta x$</span>。 但在高维空间，在流形（Manifold）上，在咱们做深度学习优化的时候，<span class="math-tex" data-tex="dx">$dx$</span> 有一个非常高大上的名字：<b>余切向量（Cotangent Vector）</b>，或者是 <b>1-形式（1-form）</b>。</p><p data-pid="MQymKlTu">别被名字吓跑。咱们用算法的逻辑拆解它。</p><p data-pid="AnfZ-9KT">假设你有一个向量空间 <span class="math-tex" data-tex="V">$V$</span>（比如切空间，想象成切线方向）。 在这个空间里，有两个主要的角色：</p><ol style="list-style-type: decimal; padding-left: 1.5em; margin-left: 0px; margin-top: 0.5em; margin-bottom: 0.5em;"><li data-pid="TuW9P1h1" style="display: list-item; margin-top: 0.25em; margin-bottom: 0.25em;"><b>向量（Vector）</b>：比如速度、位移。咱们记作 <span class="math-tex" data-tex="v">$v$</span>。</li><li data-pid="TWIEyg1m" style="display: list-item; margin-top: 0.25em; margin-bottom: 0.25em;"><b>对偶向量（Dual Vector）</b>：这是一个函数，它吃进一个向量，吐出一个数。这叫 线性泛函。</li></ol><p data-pid="x5Y6m6QA"><b><span class="math-tex" data-tex="dx">$dx$</span> 就是一个对偶向量。</b></p><p data-pid="BX1G55GA">它的作用是啥？它的作用是 <b>测量</b>。 <span class="math-tex" data-tex="dx">$dx$</span> 是一个尺子，它量的是向量在 <span class="math-tex" data-tex="x">$x$</span> 轴上的投影长度。 <span class="math-tex" data-tex="dy">$dy$</span> 是另一个尺子，它量的是向量在 <span class="math-tex" data-tex="y">$y$</span> 轴上的投影长度。</p><p data-pid="RsyWJHAE">所以，<span class="math-tex" data-tex="df = \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy">$df = \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy$</span> 这个公式的意思是： 我们要计算函数 <span class="math-tex" data-tex="f">$f$</span> 的变化量（<span class="math-tex" data-tex="df">$df$</span>），怎么算？ 我们把变化量分解成两部分： 第一部分是 <span class="math-tex" data-tex="x">$x$</span> 方向的变化率（偏导数）乘以 <span class="math-tex" data-tex="x">$x$</span> 方向的投影长（<span class="math-tex" data-tex="dx">$dx$</span>）。 第二部分是 <span class="math-tex" data-tex="y">$y$</span> 方向的变化率乘以 <span class="math-tex" data-tex="y">$y$</span> 方向的投影长（<span class="math-tex" data-tex="dy">$dy$</span>）。</p><p data-pid="8qF5K_za"><b>这是一个内积！</b> 或者说是梯度向量和位移向量的点积。</p><p data-pid="loA6axBB">教材不明不白的地方在于，它不告诉你 <span class="math-tex" data-tex="dx">$dx$</span> 是基底。 在对偶空间里，<span class="math-tex" data-tex="\{dx, dy\}">$\{dx, dy\}$</span> 是一组 基底（Basis）。任意一个微分 <span class="math-tex" data-tex="df">$df$</span> 都是这组基底的线性组合。</p><p data-pid="R7zYHjM6">这就像我们在图像处理里，把一张图分解成 RGB 三个通道。<span class="math-tex" data-tex="dx">$dx$</span> 就是 R 通道，<span class="math-tex" data-tex="dy">$dy$</span> 就是 G 通道。导数 <span class="math-tex" data-tex="\frac{\partial f}{\partial x}">$\frac{\partial f}{\partial x}$</span> 就是 R 通道的像素值（系数）。</p><p data-pid="YWqVwmS5">这套理论在 <b>张量分析</b> 和 <b>微分几何</b> 里是基石。但在大一高数里，老师不敢讲，怕学生当场退学。于是就变成了“它是一个很小的数”。</p><p data-pid="KJJnCl0O">如果你对这个视角感兴趣，想看看真正的数学大厦长什么样，推荐你去翻翻 <b>Loring Tu的《An Introduction to Manifolds》</b> 的前几章。不用全看，看懂切空间和微分形式那部分就行。你会发现，原来 <span class="math-tex" data-tex="d">$d$</span> 是一个 <b>外微分算子</b>，满足 <span class="math-tex" data-tex="d^2 = 0">$d^2 = 0$</span>。这个性质 <span class="math-tex" data-tex="d^2=0">$d^2=0$</span> 甚至直接关联到拓扑学里的 德拉姆上同调。看，这才是严谨体系的味道。</p><p class="ztext-empty-paragraph"><br></p><p data-pid="QKzAk2WJ">你问到了高阶微分 <span class="math-tex" data-tex="d^n y">$d^n y$</span>。这绝对是普通教材最 坑爹 的地方。</p><p data-pid="Il5_fEvP">很多教材会给你推导：<span class="math-tex" data-tex="d^2 y = f''(x) (dx)^2">$d^2 y = f''(x) (dx)^2$</span>。 然后告诉你，这玩意儿形式不变性只在 <span class="math-tex" data-tex="x">$x$</span> 是自变量时成立。如果 <span class="math-tex" data-tex="x">$x$</span> 是中间变量，这公式就不成立了。</p><p data-pid="SBjXljzx">这叫什么严谨体系？这叫打补丁！ 一个数学定义，居然还依赖于变量是“自变量”还是“中间变量”这种物理身份？简直是离谱。</p><p data-pid="gfgzAxgp"><b>事实是：在现代微分几何中，通常根本不定义高阶微分 <span class="math-tex" data-tex="d^2 f">$d^2 f$</span> 这种东西。</b></p><p data-pid="D_K0qXs-">因为 <span class="math-tex" data-tex="df">$df$</span> 是一个 1-形式（1-form）。对 1-form 再求外微分 <span class="math-tex" data-tex="d(df)">$d(df)$</span>，根据外微分的性质，结果恒等于 0。 所以在流形上，没有二阶微分这个概念。</p><p data-pid="KRDD_Snv">我们在优化算法里用的 二阶导数（海森矩阵 Hessian Matrix），它不是一个微分形式，它是一个 <b>双线性形式</b>。 教材为了强行把 <span class="math-tex" data-tex="f''(x)">$f''(x)$</span> 写成 <span class="math-tex" data-tex="d^2y / dx^2">$d^2y / dx^2$</span> 的形式，搞出了一套非常不自然的定义。这纯粹是为了凑那个莱布尼茨的记号。</p><p data-pid="Q6rNm4XF">我的建议是：<b>忘掉高阶微分。</b> 在实际工作中，无论是做金融风控模型，还是做自动驾驶的轨迹规划，我们只用 <b>泰勒展开</b>。 <span class="math-tex" data-tex="f(x+h) \approx f(x) + f'(x)h + \frac{1}{2} h^T H h">$f(x+h) \approx f(x) + f'(x)h + \frac{1}{2} h^T H h$</span> 这里 <span class="math-tex" data-tex="H">$H$</span> 是海森矩阵。这就够了。 别去纠结 <span class="math-tex" data-tex="d^2 y">$d^2 y$</span> 到底是个什么对象，它在现代数学框架里是一个 畸形儿。</p><p data-pid="hfn9VSHS"><b>那么为什么很多教科书都讲不清楚？</b>因为这里面有一个巨大的 认知断层。</p><p data-pid="oqv5-SfU">在这个断层的一边，是 <b>古典分析</b>（17-19世纪）。代表人物是牛顿、莱布尼茨、欧拉。他们的直觉极强，物理意义明确，但是逻辑上有漏洞。目前的工科微积分（Calculus）主要教这个。因为好用，能算题，能造桥。</p><p data-pid="frJSbWqf">在断层的另一边，是 <b>现代分析</b>（20世纪以后）。代表是流形、泛函、拓扑。这里面逻辑严丝合缝，但抽象程度极高。这里讲的是 Analysis（数学分析）甚至 Manifolds。</p><p data-pid="Ho3Qmx4y"><b>大多数教科书试图用古典的语言去解释现代的逻辑，或者用现代的逻辑去圆古典的符号。</b> 结果就是两头不讨好。讲严谨了，工科生听不懂；讲直觉了，理科生觉得你在扯淡。</p><p data-pid="pSN4kbY9"><b>六、 给你的一点过来人的建议</b></p><p data-pid="Sg48HXtS">既然你问了这个问题，说明你不是一个满足于 当调包侠 的人。你想搞懂底层的 源码。这非常好。</p><p data-pid="VXKTmU93">在IT行业，尤其是搞算法这块，这种 <b>Deep Dive</b> 的能力是核心竞争力。别人只会调 <code>sklearn.linear_model</code>，你知道梯度下降背后的几何意义，你知道海森矩阵怎么影响收敛速度，你知道正则化其实是引入了先验分布。这就是区别。</p><p data-pid="QWYjheu3">针对你的困惑，我有几条非常具体的建议，不玩虚的：</p><ol style="list-style-type: decimal; padding-left: 1.5em; margin-left: 0px; margin-top: 0.5em; margin-bottom: 0.5em;"><li data-pid="1zWI5jlA" style="display: list-item; margin-top: 0.25em; margin-bottom: 0.25em;"><b>把微分看作 线性逼近，而不是无穷小。</b> 以后看到 <span class="math-tex" data-tex="dy">$dy$</span>，脑子里直接翻译成：切线（或者切平面）上的增量。 看到 <span class="math-tex" data-tex="dx">$dx$</span>，翻译成：一个把向量映射为它第一个坐标分量的线性函数。</li><li data-pid="hJjZ0eDQ" style="display: list-item; margin-top: 0.25em; margin-bottom: 0.25em;"><b>看一看现代视角的实战应用：自动微分（Automatic Differentiation）。</b><br>你在用 PyTorch 或 TensorFlow 的时候，写了一行 <code>loss.backward()</code>。 机器是怎么算出梯度的？ 它不是在做 lim⁡Δx→0limΔ<i>x</i>→0​，那是数值微分，误差大且慢。 它做的是 <b>计算图</b>（Computational Graph） 的 <b>链式法则</b> 传播。 在这里，每一个节点都是一个运算，每一条边传递的都是 <b>雅可比矩阵</b>（或者向量-雅可比积）。<br>如果你想从代码层面彻底搞懂这个动态过程，建议去读一读 <a href="https://mp.weixin.qq.com/s/AiOemKLE_pbvAMwHcrjUng" class=" wrap external" target="_blank" rel="nofollow noreferrer">《动手学深度学习》(Dive into Deep Learning)</a>，这就是圈内大名鼎鼎的 <b>d2l</b>。李沐老师他们搞的，真的是保姆级教程，有理论有代码。或者如果你想更深入地看 PyTorch 是如何实现这些数学概念的，可以看看 <a href="https://mp.weixin.qq.com/s/vYts8Gl53mkkj70EriYtWg" class=" wrap external" target="_blank" rel="nofollow noreferrer">动手学PyTorch建模与应用</a>，你会发现那些抽象的数学符号在代码里变得无比具体。</li><li data-pid="H_1bFS_W" style="display: list-item; margin-top: 0.25em; margin-bottom: 0.25em;"><b>建立计算机科学的底层大局观。</b><br>你现在的困惑，本质上是对“黑盒”的不满足。要真正理解这些数学工具如何在计算机上跑起来，你需要补充一些硬核的计算机基础。我非常推荐你花时间看看这份 <a href="https://mp.weixin.qq.com/s/igbCF4F-K0DJDxVrMJ9ViQ" class=" wrap external" target="_blank" rel="nofollow noreferrer">全网累计下载100w+次，瞬间让你起飞的计算机基础知识</a>，把数据结构、算法和底层原理串起来。当你理解了计算机是如何存储浮点数、如何进行数值计算的，你对无穷小和误差的理解会上升一个维度。</li><li data-pid="dPaQUUJt" style="display: list-item; margin-top: 0.25em; margin-bottom: 0.25em;"><b>别死磕某些国内教材的定义。</b> 有些教材的定义是为了 考试 服务的。如果你发现逻辑不通，多半是书写得烂。直接去查维基百科的英文词条，或者翻阅 <b>GTM (Graduate Texts in Mathematics)</b> 系列里的基础书。比如 <b>Rudin 的《Principles of Mathematical Analysis》</b>（著名的蓝皮书），虽然难，但是它绝对不会骗你。它不会跟你说无穷小，它会跟你说拓扑、测度。</li><li data-pid="JCsmfe7S" style="display: list-item; margin-top: 0.25em; margin-bottom: 0.25em;"><b>实际案例佐证：自动微分（Automatic Differentiation）。</b> 你在用 PyTorch 或 TensorFlow 的时候，写了一行 <code>loss.backward()</code>。 机器是怎么算出梯度的？ 它不是在做 <span class="math-tex" data-tex="\lim_{\Delta x \to 0}">$\lim_{\Delta x \to 0}$</span>，那是数值微分，误差大且慢。 它做的是 <b>计算图（Computational Graph）</b> 的 链式法则 传播。 在这里，每一个节点都是一个运算，每一条边传递的都是 <b>雅可比矩阵</b>（或者向量-雅可比积）。 这就是微分的现代应用：<b>组合函数的线性化传递</b>。 你要是理解了这一点，你就理解了为什么深度学习能 work。因为它本质上就是把一个巨大的复杂函数，分解成无数个简单的函数（加减乘除、ReLU），然后利用 微分（线性逼近） 的链式法则，把误差传回来。</li></ol><p class="ztext-empty-paragraph"><br></p><p data-pid="c7oQIBnD">教科书不明不白，是因为它们背负了太沉重的历史包袱，试图在给非数学专业的学生科普时，牺牲了逻辑的严密性。</p><p data-pid="bm6hcmpt">这不是你的错。</p><p data-pid="-abYis3g">对于我们这些在行业里摸爬滚打的人来说，<b>d 就是 Linearization（线性化），dx 就是 Basis of Dual Space（对偶基底）。</b></p><p data-pid="c1_qmO6j">建立起 线性代数 的视角，抛弃 无穷小 的执念。你会发现，数学其实比教科书上写的要干净漂亮得多。</p><p data-pid="KdJNVxww">在这个信息爆炸的时代，能沉下心来问这种 原理 性问题的人不多了。保持这份好奇心和钻牛角尖的劲头。当年我死磕这些概念的时候，觉得百无一用，后来做复杂系统建模和高维优化算法的时候，这些直觉救了我的命。有些东西，只有当你理解了最底层的逻辑，才能在上面建起万丈高楼。</p></span><span id="VirtualCatalogAnchorPoint"></span></span></div></div>
        </div>
</body>
</html>